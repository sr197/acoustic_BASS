---
title: "Upload Chorus Frog Random Forest Model
author: "Emily Chen"
date: "2023-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Setup

## 1.1 Load packages

```{r}
devtools::install_github("https://github.com/DenaJGibbon/behaviouR") 

devtools::install_github("https://github.com/DenaJGibbon/gibbonR")
#this is Dena Clink's book of tutorials - https://bookdown.org/djc426/behaviouR-R-package-tutorials/ 
library(behaviouR)

library(ggfortify)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(tidyflow)
library(tidyverse)

install.packages("soundClass")

```

Spectogram

```{r}
SpectrogramSingle(sound.file = "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/acoustic_BASS/data/subset/present1.wav")

#change the frequency 
SpectrogramSingle(sound.file = "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/acoustic_BASS/data/subset/present1.wav", min.freq = 500, 
    max.freq = 2500,
    Colors = "Colors")
```

From this spectogram, we can tell which frequency the actual call is between (500-2500)

```{r,importing data and data wrangling}
Pfer_raw <- read.csv("../data/Pfer_raw.csv")
getwd()
# Assuming your date column is named "Date" and is in the format "yyyy-mm-dd"
Pfer_raw$Date <- as.Date(Pfer_raw$date, format = "%m/%d/%Y")

Pfer_raw <- Pfer_raw %>% 
  rename(Validation = val.Pseudacris.feriarum.Common.Song.) %>% 
  rename(Site = site) %>% 
  rename(Filename = filename)

# Deleting columns that are not necessary
Pfer_clean <- Pfer_raw %>% 
  select(Filename, Site, Date, Validation)
  
# Convert the column to a factor with custom levels
Pfer_clean$Validation <- factor(Pfer_clean$Validation, levels = c(1, 0), labels = c("present", "absent"))

# Write the updated data frame back to a CSV file
write.csv(Pfer_clean, "../data/Pfer_clean", row.names = FALSE)
```

```{r}

# vars <- read.csv("arbimonCnictraining.csv") #this tells me which ones are present/absent

SpectrogramSingle(sound.file = "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_21/PAM2_20230307_214000.wav")

# change the frequency 
SpectrogramSingle(sound.file = "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_21/PAM2_20230307_214000.wav", min.freq = 2500, 
    max.freq = 3500,
    Colors = "Colors")
```

# 2 Model

## 2.1 feature extraction

Create subset of files in CSV & in Site_41 based on whether they have been classified already.

```{r}
# Specify the paths for the old and new directories
old_directory <- "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_41"
new_directory <- "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_41_subset"

# Create the new directory if it doesn't exist
if (!file.exists(new_directory)) {
  dir.create(new_directory)
}

# Get a list of files in the old directory
all_files <- list.files(old_directory, pattern = ".wav", full.names = TRUE)

# Filter files based on the conditions in Pfer_clean dataframe
files_in_dataframe <- Pfer_clean$Filename[Pfer_clean$Validation %in% c("present", "absent") & Pfer_clean$Site == "Site_41"]

# Create a subset of files based on the condition
file_subset <- all_files[basename(all_files) %in% files_in_dataframe]

# Copy the subset of files to the new directory
for (file_path in file_subset) {
  new_file_path <- file.path(new_directory, basename(file_path))
  file.copy(file_path, new_file_path)
  cat("Copied file:", file_path, "to", new_file_path, "\n")
}
```

MFCCFunction Truncates filename in `Class` after an underscore, so the below code renames files based on absent/present.

```{r}
# Assuming dir_path is the path to the directory containing the files
dir_path <- "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_41_subset"

# Get the list of files in the directory
file_list <- list.files(dir_path)

# Merge Pfer_clean with file_list based on Filename
merged_data <- merge(Pfer_clean, data.frame(Filename = file_list), by = "Filename")

# Function to generate new filenames
generate_new_filename <- function(row, index) {
  paste0(row$Validation, "_", index, ".wav")
}

# Rename files
for (i in 1:nrow(merged_data)) {
  old_filename <- file.path(dir_path, merged_data$Filename[i])
  new_filename <- generate_new_filename(merged_data[i, ], i)
  new_filename <- file.path(dir_path, new_filename)

  # Rename the file
  file.rename(old_filename, new_filename)
}

# Optionally, print the renamed files
list.files(dir_path)
```

```{r}
MyFeatureDataFrame <- MFCCFunction(input.dir ="~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_41_subset", min.freq = 2500,max.freq = 3500)

# Cnic_training70_MFCCF <- gibbonR::MFCCFunction(input.dir=audio_list_Cnic70, min.freq = 100, max.freq = 1800,win.avg='standard')

# input.dir2 = "~/Library/CloudStorage/Box-Box/innovations-research-tech/PAM/DukeForestAcousticData/Site_21"
```

This is a multivariate data frame that we can now cluster using a pca

## 2.2 PCA

Cluster with PCA

```{r}
pca_res <- prcomp(MyFeatureDataFrame[,-c(1)], scale. = TRUE)

# MyFeatureDataFrame <- MyFeatureDataFrame %>%
#  left_join(Pfer_clean %>% select(Filename, Validation), by = c("Class" = "Filename"))

ggplot2::autoplot(pca_res, data = MyFeatureDataFrame,
         colour = 'Class')

```

## 2.3 model runs

Run a basic random forest and support vector machine

```{r}

MyFeatureDataFrame <- MyFeatureDataFrame
MyFeatureDataFrame$Class <- as.factor(MyFeatureDataFrame$Class)

ml.model.rf <- randomForest::randomForest(x=MyFeatureDataFrame[, 2:ncol(MyFeatureDataFrame)], y = MyFeatureDataFrame$Class)


print(ml.model.rf)

```

# 3 cross validation

I think the best way to do this would be using tidymodels. For the general workflow we are going to split our data up into 70% for training/tuning the model and 30% for testing. That 30% is going to be locked in a vault and we won't look at it until we have created a model we are satisfied with using cross validation (on the 70%)

as an example, I will work with 10 data files

## 3.1 split into training and testing

training = 70%, testing (vault) = 30% remember, within the training data we will be doing some sort of cross validation - if you aren't familiar with tidymodels, refer to the separate code called tidymodels_regression_trees.Rmd

The function rsample::initial_split() takes the original data and saves the information on how to make the partitions.

Here we used the strata argument, which conducts a stratified split. This ensures that, despite any imbalance in our class variable, our training and test data sets will keep roughly the same proportions as in the original data. After the initial_split, the training() and testing() functions return the actual data sets.

```{r}
set.seed(123)
rf_split <- initial_split(MyFeatureDataFrame, 
                            strata = Class, 
                          prop = .7)

train <- training(rf_split)
test  <- testing(rf_split)
```

## 3.2 Random forest

To fit a random forest model on the training set, let's use the parsnip package with the ranger engine. We first define the model that we want to create:

```{r}
# Define the random forest
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# Starting with this parsnip model object, the fit() function can be used with a model formula. Since random forest models use random numbers, we again set the seed prior to computing:

# Define the `tidyflow` with the random forest model
# and include all variables
set.seed(234)
rf_fit <- 
  rf_mod %>% 
  fit(Class ~ ., data = train)
rf_fit
```

This new rf_fit object is our fitted model, trained on our training data set.

But how do we know this is the best model? Lets use v-fold cross validation and tuning- read more about that here: <https://www.tidymodels.org/start/resampling/> and here: <https://juliasilge.com/blog/sf-trees-random-tuning/>

## 3.3 Tuning

Now it's time to create a model specification for a random forest where we will tune mtry (the number of predictors to sample at each split) and min_n (the number of observations needed to keep splitting nodes). These are hyperparameters that can be learned from data when training the model.

```{r}
tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

tune_spec
```

Now it's time to tune the hyperparameters for a random forest model. First, let's create a set of cross-validation resamples to use for tuning.

```{r}
set.seed(234)
trees_folds <- vfold_cv(train,v = 2, strata = Class) #usually you use something like 10-fold cross validation 
```

Use tune_grid() to fit models at all the different values we chose for each tuned hyperparameter.

Here we use a workflow() with a straightforward formula; if this model required more involved data preprocessing, we could use add_recipe() instead of add_formula().

```{r}
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(Class ~ .)

```

We can't learn the right values when training a single model, but we can train a whole bunch of models and see which ones turn out best. We can use parallel processing to make this go faster, since the different parts of the grid are independent. Let's use grid = 20 to choose 20 grid points automatically.

```{r}
doParallel::registerDoParallel()

set.seed(345)
tune_res <- tune_grid(
  tree_wf,
  resamples = trees_folds,
  grid = 20
)

tune_res
```

Once we have our tuning results, we can both explore them through visualization and then select the best result. The function collect_metrics() gives us a tidy tibble with all the results. We had two metrics, accuracy and roc_auc, and we get a row for each metric and model.

```{r}
tune_res %>% 
  collect_metrics()
```

We might get more out of plotting these results:

```{r}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

we can go back and tune again if we know that there is a range of min_n and mtry that work best

```{r}
rf_grid <- grid_regular(
  mtry(range = c(0, 30)),
  min_n(range = c(0, 15)),
  levels = 5
)

rf_grid
```

We can tune one more time, but this time in a more targeted way with this rf_grid.

```{r}
set.seed(456)


regular_res <- tune_grid(
  tree_wf,
  resamples = trees_folds,
  grid = rf_grid
)

regular_res

regular_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

## 3.4 best model

```{r}
best_auc <- select_best(tune_res, "roc_auc") #you could do this on regular_res as well 

final_rf <- finalize_model(
  tune_spec,
  best_auc
)

final_rf
```

## 3.5 testing

Let's make a final workflow, and then fit one last time, using the convenience function last_fit(). This function fits a final model on the entire training set and evaluates on the testing set. We just need to give this function our original train/test split.

```{r}
final_wf <- workflow() %>%
  add_formula(Class ~ .) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(rf_split)

final_res %>%
  collect_metrics()

#confusion matrix 
final_res %>%
    collect_predictions() %>%
    conf_mat(Class, .pred_class)

```
